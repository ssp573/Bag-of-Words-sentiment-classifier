{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Ok, everybody agreed on what was the best season. The first. And killing off Boone was a bad desicion. Also killing off others was bad. Blame the directors and writers for it. Bad boys. BUT. I still think this is the best scifi series ever! Sorry guys I can't help it! I see that the quality of the series was decreasing after the first season. Still it's easy to accept Liam as the new main character, if you are over Boone. He is really... mysterious. The thing that shocked me most was when Lilli was written out of the story and how. That was something she didn't deserve! And what do we get? Some blonde chick called Renee, with absolutely no character! But these Taelons stay mysterious, and you stay wondering about theyre true plans till the end. True Suspence. The conversations between Zo'or and Da'an are sometimes brilliant.<br /><br />I understand that, when you jump in on an episode from the 3th,4th or 5th season, you may not understand this show. But when you watch from the beginning, you just cant break loose!<br /><br />The acting is great, the special FX are marvellous, the music is beautiful and the plot intriguing. Gotta see this, guys!\",\n",
       "  'sentiment': 'pos'},\n",
       " {'text': \"This is one of those movies the critics really missed the mark on. This movie is practically McHale's Navy for the 90s or Police Academy at sea. Grammer proves he can play roles other than Frasier as he outwits and outfoxes the Navy in order to get his own sub. Rob Schneider is as wormy as usual, the same in every role he plays, and Lauren Holly is the local sexpot albeit with a brain. Ken Hudson Campbell is as funny as usual with almost every line a catch phrase. The movie has a wonderful intelligent plot and a non-predictable script that still surprises me every time I watch it. Many of the Navy phrases and terms go over my head, though, but it's a small obstacle for the sheer accuracy and realism of the movie and its characters.\",\n",
       "  'sentiment': 'pos'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split=20000\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "train = imdb_dataset(train=True)\n",
    "test= imdb_dataset(test=True)\n",
    "train[:2]\n",
    "test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "pkl.dump(train,open(\"train.pkl\",\"wb\"))\n",
    "pkl.dump(test,open(\"test.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "Justifications for what happened to his movie in terms of distributors and secondary directors, drunks and receptionists doing script rewrites aside, let's just take this movie as it's offered, without extraneous explanations.<br /><br />This movie is God awful. Straight up craptastic. Rather than rehash what may serve as a plot, I'll run a highlight reel of some curious points that made me scratch my head.<br /><br />A class (of 5) take a field trip for a history class to the middle of friggin' nowhere Ireland. These students may be Canadian or American, it's difficult to tell. That it was filmed in a Canadian forest rather than Ireland is rather obvious as well. One student seems to know nothing about history and is basically the \"dumb jock\" character from a number of kick ass 80's movie, except when he channels Randy from Scream. One character may be Chris Klein's stunt double. He has a girlfriend who probably gets killed, but it's never really established if that is true. One character is sullen and removed from her peers...just...cuz... and then there's a blonde girl. Yay blonde girl.<br /><br />Ireland has a population of 2. They're cousins. Gary, who is clearly the same age or younger than the rest of the cast, is called \"sir\" more than once. He's very ominous and wears a knit cap. His cousin is a roughed up porn star with the worst Irish accent to befoul film in my lifetime and most likely beyond.<br /><br />Picturesque Ireland features many Canadian forests and swampy areas and 2 ducks which appear more than once in cut scenes.<br /><br />The producers got a discount on volume fake entrails. Good for them.<br /><br />Unbeknownst to me, horribly inbred freaks have access to brand spanking new hunting knives. Perhaps there's some kind of outdoorsman outlet nearby with a blind and deaf clerk working the register.<br /><br />Also unbeknownst to me, if you inbreed for roughly 600 years, as the story leads us to believe happened, you end up being somewhat lumpy, yet amazingly spry and fairly strong. Genetics are a wonderful game of craps.<br /><br />There may or may not be more than one freak in this film. Reference is made to \"them\" and we see shadows, yet only one odd looking dude is seen ever. And when one odd looking dude is finally killed, apparently all danger is passed. I'm running with my initial assumption that no one thought to outfit a second man in full make up, thus they just used the one. That's what it looks like on screen, anyway.<br /><br />Richard Grieco should be ashamed.<br /><br />Also of note, aside from those shiny new knives, the inbred freaks have access to some posh leather gear, as once Richard Grieco cuts his bonds, there are fresh ones ready for the next sucker who gets tied up...who also then escapes, because the chains give you enough slack to just undo them, making one wonder why they even bother tying anyone up.<br /><br />A dead body in a shack will be maggot-ridden after what I would guess is about 2 hours has passed. Said dead body will also have glasses on, when no characters wore them. Curious.<br /><br />Jenna Jameson appears for no reason from stage left, chats for 2 minutes, vanishes stage left. In the middle of a giant forest. That's not unusual, as Gary can also pop out of nowhere, which is also known as whatever exists in TV land off the screen.<br /><br />Ms. Jameson dies sadly and somehow her clothes vanish like my hopes that this movie wouldn't suck wind.<br /><br />I offer a special nod to the \"Breeder\" character, the poor girl who has been used by the freaks for months (or maybe years) for breeding purposes. The poor girl who still has eye shadow on and emotes on camera with all the passion and conviction of a stuffed chihuahua.<br /><br />The ending of this movie was clearly tacked on by a drunk or someone with a fierce mental disability that has been cultivated and encouraged with excessive gasoline drinking over the years.<br /><br />Apparently this wasn't just random crap I found on the movie network late at night, apparently people have heard of and even followed this movie through it's production. How sad for you all. I have nothing more to say. May God have mercy on us all. 0\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "train=pkl.load(open(\"train.pkl\",\"rb\"))\n",
    "test=pkl.load(open(\"test.pkl\",\"rb\"))\n",
    "\n",
    "train_split=20000\n",
    "\n",
    "training=train[:train_split//2]+train[12500:12500+train_split//2]\n",
    "train_data=[]\n",
    "train_target=[]\n",
    "for sample in training:\n",
    "    train_data+=[sample['text']]\n",
    "    if sample['sentiment']=='pos':      #setting the targets to integers: 1 if positive and 0 if negative\n",
    "        train_target+=[1]\n",
    "    else:\n",
    "        train_target+=[0]\n",
    "\n",
    "validation=train[train_split//2:12500]+train[12500+train_split//2:]\n",
    "val_data=[]\n",
    "val_target=[]\n",
    "for sample in validation:\n",
    "    val_data+=[sample['text']]\n",
    "    if sample['sentiment']=='pos':\n",
    "        val_target+=[1]\n",
    "    else:\n",
    "        val_target+=[0]\n",
    "\n",
    "test_data=[]\n",
    "test_target=[]\n",
    "for sample in test:\n",
    "    test_data+=[sample[\"text\"]]\n",
    "    if sample['sentiment']=='pos':\n",
    "        test_target+=[1]\n",
    "    else:\n",
    "        test_target+=[0]\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "porter_stemmer=PorterStemmer()\n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "def tokenize(sent):\n",
    "  tokens = tokenizer(sent)\n",
    "  unwanted=set(list(punctuations)+['<br','/><br','\\n','\\t'])\n",
    "  return [porter_stemmer.stem(token.text.lower()) for token in tokens if (token.text not in unwanted)]\n",
    "  \n",
    "  #return [token.text for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing test data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "pkl.dump(val_data_tokens, open(\"val_data_tokens_stem_punct.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "pkl.dump(test_data_tokens, open(\"test_data_tokens_stem_punct.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "pkl.dump(train_data_tokens, open(\"train_data_tokens_stem_punct.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_tokens_stem_punct.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens_stem_punct.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens_stem_punct.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens_stem_punct.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens_stem_punct.p\", \"rb\"))\n",
    "\n",
    "#bigrams\n",
    "bigram_train_data_tokens=[]\n",
    "for review in train_data_tokens:\n",
    "    temp=[]\n",
    "    for i in range(len(review)-1):\n",
    "        temp.append(review[i]+\"_\"+review[i+1])\n",
    "    bigram_train_data_tokens.append(review+temp)\n",
    "\n",
    "bigram_val_data_tokens=[]\n",
    "for review in val_data_tokens:\n",
    "    temp=[]\n",
    "    for i in range(len(review)-1):\n",
    "        temp.append(review[i]+\"_\"+review[i+1])\n",
    "    bigram_val_data_tokens.append(review+temp)\n",
    "\n",
    "bigram_test_data_tokens=[]\n",
    "for review in test_data_tokens:\n",
    "    temp=[]\n",
    "    for i in range(len(review)-1):\n",
    "        temp.append(review[i]+\"_\"+review[i+1])\n",
    "    bigram_test_data_tokens.append(review+temp)\n",
    "\n",
    "temp_list=[]\n",
    "bi_gram_vocab=[]\n",
    "for i in range(len(all_train_tokens)-1):\n",
    "    temp_list.append(all_train_tokens[i]+\"_\"+all_train_tokens[i+1])\n",
    "bi_gram_vocab=all_train_tokens+temp_list\n",
    "\n",
    "#trigrams    \n",
    "trigram_train_data_tokens=[] \n",
    "trigrams_train=[]\n",
    "for review in train_data_tokens:\n",
    "    temp=[]\n",
    "    for i in range(len(review)-2):\n",
    "        temp.append(review[i]+\"_\"+review[i+1]+\"_\"+review[i+2])\n",
    "    trigrams_train.append(temp)\n",
    "for i in range(len(bigram_train_data_tokens)):\n",
    "    trigram_train_data_tokens.append(bigram_train_data_tokens[i]+trigrams_train[i])\n",
    "    \n",
    "\n",
    "trigram_val_data_tokens=[] \n",
    "trigrams_val=[]\n",
    "for review in val_data_tokens:\n",
    "    temp=[]\n",
    "    for i in range(len(review)-2):\n",
    "        temp.append(review[i]+\"_\"+review[i+1]+\"_\"+review[i+2])\n",
    "    trigrams_val.append(temp)\n",
    "for i in range(len(bigram_val_data_tokens)):\n",
    "    trigram_val_data_tokens.append(bigram_val_data_tokens[i]+trigrams_val[i])\n",
    "\n",
    "trigram_test_data_tokens=[] \n",
    "trigrams_test=[]\n",
    "for review in test_data_tokens:\n",
    "    temp=[]\n",
    "    for i in range(len(review)-2):\n",
    "        temp.append(review[i]+\"_\"+review[i+1]+\"_\"+review[i+2])\n",
    "    trigrams_test.append(temp)\n",
    "for i in range(len(bigram_test_data_tokens)):\n",
    "    trigram_test_data_tokens.append(bigram_test_data_tokens[i]+trigrams_test[i])\n",
    "    \n",
    "temp_list_tri=[]\n",
    "tri_gram_vocab=[]\n",
    "for i in range(len(all_train_tokens)-2):\n",
    "    temp_list_tri.append(all_train_tokens[i]+\"_\"+all_train_tokens[i+1]+\"_\"+all_train_tokens[i+2])\n",
    "tri_gram_vocab=bi_gram_vocab + temp_list_tri\n",
    "\n",
    "\n",
    "#fourgrams\n",
    "    \n",
    "fourgram_train_data_tokens=[] \n",
    "fourgrams_train=[]\n",
    "for review in train_data_tokens:\n",
    "    temp=[]\n",
    "    for i in range(len(review)-3):\n",
    "        temp.append(review[i]+\"_\"+review[i+1]+\"_\"+review[i+2]+\"_\"+review[i+3])\n",
    "    fourgrams_train.append(temp)\n",
    "for i in range(len(trigram_train_data_tokens)):\n",
    "    fourgram_train_data_tokens.append(trigram_train_data_tokens[i]+fourgrams_train[i])\n",
    "    \n",
    "\n",
    "fourgram_val_data_tokens=[] \n",
    "fourgrams_val=[]\n",
    "for review in val_data_tokens:\n",
    "    temp=[]\n",
    "    for i in range(len(review)-3):\n",
    "        temp.append(review[i]+\"_\"+review[i+1]+\"_\"+review[i+2]+\"_\"+review[i+3])\n",
    "    fourgrams_val.append(temp)\n",
    "for i in range(len(trigram_val_data_tokens)):\n",
    "    fourgram_val_data_tokens.append(trigram_val_data_tokens[i]+fourgrams_val[i])\n",
    "\n",
    "fourgram_test_data_tokens=[] \n",
    "fourgrams_test=[]\n",
    "for review in test_data_tokens:\n",
    "    temp=[]\n",
    "    for i in range(len(review)-3):\n",
    "        temp.append(review[i]+\"_\"+review[i+1]+\"_\"+review[i+2]+\"_\"+review[i+3])\n",
    "    fourgrams_test.append(temp)\n",
    "for i in range(len(bigram_test_data_tokens)):\n",
    "    fourgram_test_data_tokens.append(trigram_test_data_tokens[i]+fourgrams_test[i])\n",
    "    \n",
    "temp_list_four=[]\n",
    "four_gram_vocab=[]\n",
    "for i in range(len(all_train_tokens)-3):\n",
    "    temp_list_four.append(all_train_tokens[i]+\"_\"+all_train_tokens[i+1]+\"_\"+all_train_tokens[i+2]+\"_\"+all_train_tokens[i+3])\n",
    "four_gram_vocab=tri_gram_vocab + temp_list_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 19151471\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(bigram_train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(bigram_val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(bigram_test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(four_gram_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)\n",
    "#token2id, id2token = build_vocab(bi_gram_vocab)\n",
    "#token2id, id2token = build_vocab(tri_gram_vocab)\n",
    "#token2id, id2token = build_vocab(four_gram_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 983 ; token otherwis\n",
      "Token otherwis; token id 983\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader randomly selects batch of data from the dataset\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImdbDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        #print(len(self.data_list),len(self.target_list))\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def imdb_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    #print(label_list[:10])\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_dataset = ImdbDataset(train_data_indices, train_target)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = ImdbDataset(val_data_indices, val_target)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = ImdbDataset(test_data_indices, test_target)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=imdb_collate_func,\n",
    "                                           shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        #embed dimension should be atleast 100\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "#print(len(id2token))\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/1250], Validation Acc: 57.54\n",
      "Epoch: [1/10], Step: [201/1250], Validation Acc: 67.2\n",
      "Epoch: [1/10], Step: [301/1250], Validation Acc: 70.5\n",
      "Epoch: [1/10], Step: [401/1250], Validation Acc: 73.52\n",
      "Epoch: [1/10], Step: [501/1250], Validation Acc: 75.52\n",
      "Epoch: [1/10], Step: [601/1250], Validation Acc: 76.54\n",
      "Epoch: [1/10], Step: [701/1250], Validation Acc: 77.62\n",
      "Epoch: [1/10], Step: [801/1250], Validation Acc: 79.58\n",
      "Epoch: [1/10], Step: [901/1250], Validation Acc: 80.94\n",
      "Epoch: [1/10], Step: [1001/1250], Validation Acc: 81.6\n",
      "Epoch: [1/10], Step: [1101/1250], Validation Acc: 82.34\n",
      "Epoch: [1/10], Step: [1201/1250], Validation Acc: 82.76\n",
      "Epoch: [2/10], Step: [101/1250], Validation Acc: 83.2\n",
      "Epoch: [2/10], Step: [201/1250], Validation Acc: 83.46\n",
      "Epoch: [2/10], Step: [301/1250], Validation Acc: 83.66\n",
      "Epoch: [2/10], Step: [401/1250], Validation Acc: 84.06\n",
      "Epoch: [2/10], Step: [501/1250], Validation Acc: 84.3\n",
      "Epoch: [2/10], Step: [601/1250], Validation Acc: 84.44\n",
      "Epoch: [2/10], Step: [701/1250], Validation Acc: 84.72\n",
      "Epoch: [2/10], Step: [801/1250], Validation Acc: 85.1\n",
      "Epoch: [2/10], Step: [901/1250], Validation Acc: 85.02\n",
      "Epoch: [2/10], Step: [1001/1250], Validation Acc: 85.3\n",
      "Epoch: [2/10], Step: [1101/1250], Validation Acc: 85.26\n",
      "Epoch: [2/10], Step: [1201/1250], Validation Acc: 85.54\n",
      "Epoch: [3/10], Step: [101/1250], Validation Acc: 85.58\n",
      "Epoch: [3/10], Step: [201/1250], Validation Acc: 85.72\n",
      "Epoch: [3/10], Step: [301/1250], Validation Acc: 85.6\n",
      "Epoch: [3/10], Step: [401/1250], Validation Acc: 85.82\n",
      "Epoch: [3/10], Step: [501/1250], Validation Acc: 85.92\n",
      "Epoch: [3/10], Step: [601/1250], Validation Acc: 85.92\n",
      "Epoch: [3/10], Step: [701/1250], Validation Acc: 86.0\n",
      "Epoch: [3/10], Step: [801/1250], Validation Acc: 86.06\n",
      "Epoch: [3/10], Step: [901/1250], Validation Acc: 85.94\n",
      "Epoch: [3/10], Step: [1001/1250], Validation Acc: 86.32\n",
      "Epoch: [3/10], Step: [1101/1250], Validation Acc: 86.28\n",
      "Epoch: [3/10], Step: [1201/1250], Validation Acc: 86.32\n",
      "Epoch: [4/10], Step: [101/1250], Validation Acc: 86.1\n",
      "Epoch: [4/10], Step: [201/1250], Validation Acc: 86.28\n",
      "Epoch: [4/10], Step: [301/1250], Validation Acc: 86.1\n",
      "Epoch: [4/10], Step: [401/1250], Validation Acc: 86.38\n",
      "Epoch: [4/10], Step: [501/1250], Validation Acc: 86.28\n",
      "Epoch: [4/10], Step: [601/1250], Validation Acc: 86.32\n",
      "Epoch: [4/10], Step: [701/1250], Validation Acc: 86.46\n",
      "Epoch: [4/10], Step: [801/1250], Validation Acc: 86.56\n",
      "Epoch: [4/10], Step: [901/1250], Validation Acc: 86.7\n",
      "Epoch: [4/10], Step: [1001/1250], Validation Acc: 86.52\n",
      "Epoch: [4/10], Step: [1101/1250], Validation Acc: 86.48\n",
      "Epoch: [4/10], Step: [1201/1250], Validation Acc: 86.44\n",
      "Epoch: [5/10], Step: [101/1250], Validation Acc: 86.4\n",
      "Epoch: [5/10], Step: [201/1250], Validation Acc: 86.34\n",
      "Epoch: [5/10], Step: [301/1250], Validation Acc: 86.62\n",
      "Epoch: [5/10], Step: [401/1250], Validation Acc: 86.62\n",
      "Epoch: [5/10], Step: [501/1250], Validation Acc: 86.76\n",
      "Epoch: [5/10], Step: [601/1250], Validation Acc: 86.74\n",
      "Epoch: [5/10], Step: [701/1250], Validation Acc: 86.86\n",
      "Epoch: [5/10], Step: [801/1250], Validation Acc: 86.68\n",
      "Epoch: [5/10], Step: [901/1250], Validation Acc: 86.96\n",
      "Epoch: [5/10], Step: [1001/1250], Validation Acc: 86.92\n",
      "Epoch: [5/10], Step: [1101/1250], Validation Acc: 86.82\n",
      "Epoch: [5/10], Step: [1201/1250], Validation Acc: 86.78\n",
      "Epoch: [6/10], Step: [101/1250], Validation Acc: 86.98\n",
      "Epoch: [6/10], Step: [201/1250], Validation Acc: 86.88\n",
      "Epoch: [6/10], Step: [301/1250], Validation Acc: 86.96\n",
      "Epoch: [6/10], Step: [401/1250], Validation Acc: 86.98\n",
      "Epoch: [6/10], Step: [501/1250], Validation Acc: 86.96\n",
      "Epoch: [6/10], Step: [601/1250], Validation Acc: 86.92\n",
      "Epoch: [6/10], Step: [701/1250], Validation Acc: 87.04\n",
      "Epoch: [6/10], Step: [801/1250], Validation Acc: 86.9\n",
      "Epoch: [6/10], Step: [901/1250], Validation Acc: 87.02\n",
      "Epoch: [6/10], Step: [1001/1250], Validation Acc: 87.06\n",
      "Epoch: [6/10], Step: [1101/1250], Validation Acc: 87.1\n",
      "Epoch: [6/10], Step: [1201/1250], Validation Acc: 87.14\n",
      "Epoch: [7/10], Step: [101/1250], Validation Acc: 87.24\n",
      "Epoch: [7/10], Step: [201/1250], Validation Acc: 87.04\n",
      "Epoch: [7/10], Step: [301/1250], Validation Acc: 87.28\n",
      "Epoch: [7/10], Step: [401/1250], Validation Acc: 87.18\n",
      "Epoch: [7/10], Step: [501/1250], Validation Acc: 87.24\n",
      "Epoch: [7/10], Step: [601/1250], Validation Acc: 86.9\n",
      "Epoch: [7/10], Step: [701/1250], Validation Acc: 87.28\n",
      "Epoch: [7/10], Step: [801/1250], Validation Acc: 87.36\n",
      "Epoch: [7/10], Step: [901/1250], Validation Acc: 87.14\n",
      "Epoch: [7/10], Step: [1001/1250], Validation Acc: 87.24\n",
      "Epoch: [7/10], Step: [1101/1250], Validation Acc: 87.4\n",
      "Epoch: [7/10], Step: [1201/1250], Validation Acc: 87.36\n",
      "Epoch: [8/10], Step: [101/1250], Validation Acc: 87.2\n",
      "Epoch: [8/10], Step: [201/1250], Validation Acc: 87.12\n",
      "Epoch: [8/10], Step: [301/1250], Validation Acc: 87.24\n",
      "Epoch: [8/10], Step: [401/1250], Validation Acc: 87.14\n",
      "Epoch: [8/10], Step: [501/1250], Validation Acc: 87.28\n",
      "Epoch: [8/10], Step: [601/1250], Validation Acc: 87.32\n",
      "Epoch: [8/10], Step: [701/1250], Validation Acc: 86.78\n",
      "Epoch: [8/10], Step: [801/1250], Validation Acc: 87.36\n",
      "Epoch: [8/10], Step: [901/1250], Validation Acc: 86.84\n",
      "Epoch: [8/10], Step: [1001/1250], Validation Acc: 87.36\n",
      "Epoch: [8/10], Step: [1101/1250], Validation Acc: 86.92\n",
      "Epoch: [8/10], Step: [1201/1250], Validation Acc: 87.28\n",
      "Epoch: [9/10], Step: [101/1250], Validation Acc: 87.26\n",
      "Epoch: [9/10], Step: [201/1250], Validation Acc: 87.18\n",
      "Epoch: [9/10], Step: [301/1250], Validation Acc: 87.08\n",
      "Epoch: [9/10], Step: [401/1250], Validation Acc: 86.98\n",
      "Epoch: [9/10], Step: [501/1250], Validation Acc: 87.18\n",
      "Epoch: [9/10], Step: [601/1250], Validation Acc: 87.1\n",
      "Epoch: [9/10], Step: [701/1250], Validation Acc: 87.2\n",
      "Epoch: [9/10], Step: [801/1250], Validation Acc: 87.2\n",
      "Epoch: [9/10], Step: [901/1250], Validation Acc: 87.24\n",
      "Epoch: [9/10], Step: [1001/1250], Validation Acc: 87.2\n",
      "Epoch: [9/10], Step: [1101/1250], Validation Acc: 87.22\n",
      "Epoch: [9/10], Step: [1201/1250], Validation Acc: 87.14\n",
      "Epoch: [10/10], Step: [101/1250], Validation Acc: 87.2\n",
      "Epoch: [10/10], Step: [201/1250], Validation Acc: 87.18\n",
      "Epoch: [10/10], Step: [301/1250], Validation Acc: 87.22\n",
      "Epoch: [10/10], Step: [401/1250], Validation Acc: 87.14\n",
      "Epoch: [10/10], Step: [501/1250], Validation Acc: 87.1\n",
      "Epoch: [10/10], Step: [601/1250], Validation Acc: 87.12\n",
      "Epoch: [10/10], Step: [701/1250], Validation Acc: 87.22\n",
      "Epoch: [10/10], Step: [801/1250], Validation Acc: 87.16\n",
      "Epoch: [10/10], Step: [901/1250], Validation Acc: 87.02\n",
      "Epoch: [10/10], Step: [1001/1250], Validation Acc: 87.08\n",
      "Epoch: [10/10], Step: [1101/1250], Validation Acc: 87.1\n",
      "Epoch: [10/10], Step: [1201/1250], Validation Acc: 87.08\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVUAAACTCAYAAAA+yoozAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xl8VOW5wPHfM0nIQjZCFhIgBDEQAxgQGmpACQK9oFIXwCKiwHUpKkWk9paqpa3aip+qrb3WDXqLyqIoKoorVhNcUcJOgLCFAElIAtn3ybz3j5lIwCQMMJP1+X4+85mZM+ec95lAnrznPec8rxhjUEop5RqW1g5AKaU6Ek2qSinlQppUlVLKhTSpKqWUC2lSVUopF9KkqpRSLqRJVSmlXEiTqlJKuZAmVaWUciHP1g7AGaGhoSYmJqa1w1AdVFpaWoExJqyZz8M9PT2XAoPQjkhnZwN2Wq3WO4YNG5bX2ArtIqnGxMSwadOm1g5DdVAicri5zz09PZf26NHjkrCwsEKLxaL3dXdiNptN8vPz43Nzc5cCP29sHf2rq9TZDQoLCyvRhKosFosJCwsrxn7U0vg6LRiPUi3OWmcjPbuEr/YXXMhuLJpQVT3H/4Umc2e7OPxXylnHS6rYklXIliNFbMkqYsfRYipr64gO8WPD/4xp7fBUJ6BJVbVblTV17DhWzNYjhWzJKmLrkSJyiqsA8PIQ4qOC+MVPejM0OpghvYNbOdrzV1BQ4LF06dKQhQsX5p/rtqNHj754zZo1h0JDQ+uaWmf+/PlRycnJpddff33phUWqQJOqaidsNsPBgnK2Hin6IYnuyS2lzmY/Ku8d4stPYkIY0juYodHBxEcF4u3p0cpRu8aJEyc8/vWvf4U3llStViuenk3/Gqempu4/2/7//ve/Z19giC2utrYWLy+v1g6jUZpUVZtTW2fjQH4Zu46VsCu7hF3ZxaTnlFBaZQUgwNuThN7B3D26H0Ojg0noHUyov3eLxPabN7f1zsgt9XPlPvv3CKj465SEI019/utf/7rXkSNHvOPi4uJHjx5dMmnSpOJHH300Mjw8vDY9Pd3vwIEDu8aNG9cvJyenS3V1tWXOnDnHH3jggQKAnj17Dt60adPukpISy8SJE2MTExPLNm3a5B8REVHz8ccf7/f39zeTJ0+Oufbaa4tnz55d2LNnz8E33XTTiY8//jjIarXK66+/fnDo0KFV2dnZnlOmTOlbVFTkOWTIkIqUlJTAtLS03ZGRkdaGsd5yyy3R27Zt61pVVWWZNGlS4d/+9rdsgNTUVL/58+dHV1RUWLp06WI2bNiwNyAgwHbPPff0SklJCQSYOXNmwUMPPZRXH3NkZKR1w4YNfg888EDv7777bu+CBQuicnJyvLKysrqEhIRYn3zyyWPTp0/vW1lZaQF45plnssaPH18O8PDDD0esXr26u4gwduzY4nvvvbdg6tSpF6Wnp+8G2LFjh/e0adMu2rVr125X/luCJlXVyipqrOzOKSU9u5hd2SWk55SwJ7eUGqsNAB8vC3E9Avl5QhQJvey90H5h/lgs0sqRt5ynnnrq6LXXXuu7Z8+edIB169YFbN++veuWLVt2xcXF1QCsWLEiMyIioq6srEyGDh0aP2PGjMIePXqcdsiflZXls3z58oNJSUmHr7766oteeeWVbvfcc8/JM9sLDQ21pqen7168eHHY4sWLI15//fXDCxcujBo9enTp448/nvvmm28Grlq1KrSxWJ9++uljERERdVarlaSkpAEbN270TUhIqLrlllv6rVix4sDo0aMrTp48afH397c99dRTYYcPH/betWtXupeXF8ePHz/rocX27dv9Nm7cuMff39+UlpZavvjiiww/Pz+zY8cO75tvvvminTt37l69enXg+++/3y0tLW1PQECA7fjx4x4RERF1AQEBdV9//bVvUlJS5Ysvvhg6ffr0E+f3L9I8TaqqxRSW1/zQ86x/PlRQjuMInmA/LwZGBTIrKYaBUYEMjAqkb6g/Hm0ogTbXo2xJl156aXl9QgV44oknIt5///1ggNzcXK9du3b59OjRo7zhNj179qxOSkqqBBg6dGhFZmZmo9376dOnFwIkJiZWvPvuu90AvvvuO/933nlnP8CUKVNKAgMDGx2jffnll0OWLVsWarVaJT8/32vbtm0+IkJ4eHjt6NGjKwBCQkJsAJ999lngnDlz8usP4yMiIpoc9603YcKEIn9/fwNQU1Mjt99+e5/09HRfi8XC4cOHvQHWr18fOGPGjIKAgABbw/3OmjWrYMmSJaGJiYlH1q5d2+377793eS8VNKkqN6qx2kg7XEhKRh6pe/PZk3vqPEjPYF/iowKZlBBFfGQgA3sGERXkg0jbSaBtmZ+fn63+9bp16wJSU1MDNm3atCcgIMCWmJg4oP6QuKEuXbr8cFmYh4eHaWwdAB8fHwPg6elprFarADgzl92ePXu6PPvssxFpaWm7w8LC6iZPnhxTVVVlMcYgIj/aQVPLPTw8jM1m/3pnxti1a9cfvvef//zniPDw8No1a9Ycstls+Pr6Dmuw3x/FN3PmzMInnngi6rXXXisdPHhwxZk9eVfR61SVS2UXVbJyYxZ3vbKJyx5dz81LvuX/vjxESNcu/HZCHCvuGMGW34/nq4VXseS24cwf15+fDexBz2BfTahNCAoKqisvL2/yd7WoqMgjKCioLiAgwLZlyxafbdu2dXV1DImJiWWvvvpqCMBbb70VWFJS8qND9cLCQg9fX19bSEhI3ZEjRzxTUlKCABISEqqOHz/eJTU11c+xnqW2tpZx48aVvPDCC2G1tbUAPxz+9+rVq+arr77yA1i9enW3pmIqLi72iIyMrPXw8OC5557rXldnz5ETJkwoefXVV0NLS0stDffr5+dnRo8eXbxgwYLoWbNmXdCFy83Rnqq6IDVWG5syT5KSkU/K3jwyjpcBEBXkw6SEKJIHhDHy4lD8vfW/2vnq0aNH3bBhw8piY2MHXnXVVcWTJk0qbvj55MmTi1966aWw/v37x/fr168qISGhvKl9na/FixdnT5ky5aL4+Phul19+eVlYWFhtcHDwaT29yy+/vHLQoEEVsbGxA6Ojo6uHDRtWBvae74oVKw7MmzcvuqqqyuLj42PbsGFDxv3335+fkZHhHRcXN9DT09PMnDkz/8EHH8xftGhR9pw5c2KeeOKJ2mHDhjX5XebPn583efLkfu+88063UaNGlfr6+trAPjyxefNmvyFDhlzi5eVlxo0bV/zss88eA7jttttOfvjhh91uvPHGElf/jOpJe5iievjw4Ubv/W87jhVVkrI3j5S9+Xy9v4Dymjq8PISfxISQPCCM5AHhxIb7t5uep4ikGWOGN/X5tm3bMhMSEtzWs2kPKisrxdPT03h5efHpp592nTt3bp/6E2ftyaJFiyKKi4s9nnnmmQu6jGzbtm2hCQkJMY19pt0HdVYNe6Of78ljX569N9oz2JfrhvYkuX8YSdob7dD279/f5aabbupns9nw8vIyL774YmZrx3Suxo8f3+/w4cPeqampGe5sR38LVKNyiitJ2Ws/pP9y3+m90ZuG9yZ5QBgXt6PeqLowgwcPrt69e3e765k2tH79+gMt0Y4mVQXYL7jffLiQzx2JtP5MfWSQDz8f0pMxA7Q3qpQz9DekE8srqfrhBNMX+woorbLiaRGG9enGwolxjBkQTv8I7Y0qdS40qXYidTbD1iOFfL4nn5SMPHYes58ADQ/wZuKgHowZEM7I2FACfdrmPdVKtQeaVDu4ypo6vtxfwPr0XP6zO48T5TVYBIb16cZv/msAyQPCiI8M1N6oUi6iF/93QAVl1az+/gh3vLyJoY9+wp2vbOLDHblc3q87z0wbwubfj+eNOUncO+ZiBkYFaULtgPz8/IYCZGZmek2YMOGixtZJTEwcsGHDhmaLwzzyyCPh9RfRg72UYEFBQcco/+Um2lPtIA7ml7E+/Tjr04+TllWIMfaTTDcN7834+AhG9O1OF0/9G9rZxMTE1H700UcHz3f7F198MeLOO+88WX8fvTOlBNsSm82GMQYPj5b7O6BJtZ2y2QxbjhQ5EmkuB/LtN55cEhnIr66K5WfxEQyM0sN6l3vn3t7kpbu09B/h8RVc/88mC7XcfffdPfv06VNTX091wYIFUQEBAXULFizInzBhwsXFxcUeVqtVFi1alD1jxoyihtvu3bu3y7XXXhu7b9++XWVlZTJt2rS+GRkZPrGxsVVVVVU//OdorGTfY489Fp6Xl+c1evTo/t26dbNu3Lgxo2FZvj/+8Y8RK1asCAW49dZb8xctWpS3d+/eLk2VGGwY18qVK4MWL14cWVtba+nWrZv19ddfP9i7d29rcXGx5fbbb4/evn27H8CDDz6YPWvWrKI333wzcNGiRT3r6uokJCTE+s0332QsWLAgyt/fv+6RRx45DhAbGztw3bp1+wAmTpwYm5SUVJqWlua/du3a/X/60596OFuScNy4cbH/+7//m1VffOayyy6Le/755w+PGDGi0pl/Tk2q7UiN1cZXBwr4eGcun+7Oo6CsGk+LMOKiEG79aR/GxUfQq5trf99V65sxY8bJ+fPnR9cn1bVr13b76KOP9vn5+dnef//9/SEhIbacnBzPESNGxE2fPr3IYmn8iOTJJ58M9/X1tWVkZKRv3LjRd+TIkfH1nzVWsu/hhx/Oe/755yNSU1Mzzqyb+sUXX/itXLmye1pa2m5jDMOGDbtk7NixpaGhoXXOlBgcP3582bRp0/ZYLBaefvrp0EceeaTHkiVLji5cuDAyMDCwLiMjIx0gPz/fIzs723Pu3LkxKSkpe+Li4mqcKRGYmZnps2TJkszly5dnNfX9mipJOGvWrIKlS5eGJiUlHdm+fbt3TU2NOJtQQZNqm1efSD/YnsPHu3IpqbLi7+3J6AFh/Cw+guT+4QT56dn6FtNMj9JdRo4cWXnixAnPzMxMr5ycHM+goKC62NjYmurqapk/f36vb7/91t9isZCXl9fl6NGjntHR0dbG9vPll1/6z5s3Lw9gxIgRlf3796+o/6yxkn3NJZKUlBT/q6++uigwMNAGcM011xR+/vnnAVOnTi1ypsTgoUOHulx//fW98vPzvWpqaiy9e/euBtiwYUPga6+99sNwRVhYWN3KlSuDEhMTS+tLHTpTIjAyMrJm7NixP9QNOJeShLNmzSr861//GlldXX30hRdeCJ0+ffo53aKsSbUNaphIP0k/TnFlLQHenoyPj+DqwZFc0T+0w0wVopwzadKkwuXLl3fLzc31mjx58kmAF198MeTEiROeO3bs2O3t7W169uw5uKlyfvUaGw5qqmRfc/tprmaIMyUG586dG33ffffl3nLLLcXr1q0LeOSRR6Lq93tmjE2V8vP09PyhRCBAdXX1Dys1LI14riUJAwICbFdccUXJypUrg999992QtLS0c7qTTM9ctBG1dTZS9ubxmze28ZM/f8rsf3/PRztzGRsXztLbhrPp9+N4+hdDGBcfoQm1E7r11ltPrlmzJmTdunXdZsyYUQj20nehoaG13t7e5r333gvIzs7u0tw+Ro0aVbZ8+fIQgO+//94nIyOjvhRfoyX7ALp27VpXXFz8ozxx1VVXlX3wwQfBpaWllpKSEssHH3zQbcyYMU5PHFhaWuoRHR1dC7Bs2bLu9cuTk5NLnn766fD69/n5+R5jxowp37hxY8CePXu6wKlSfjExMdVbt27tCvDll1/6HTt2rNGi2+dakhBgzpw5Bb/97W97JyQklDvTM25Ie6qtqLbOxlf7C/hgRw4f7zrVIx0XH8E12iNVDQwfPryqvLzcEhERUdOnT59agDvuuOPkxIkTLx40aNAlAwcOrOjbt29Vc/t44IEH8qZNm9a3f//+8QMHDqwYPHhwOTRdsg/s80ZNnDgxNjw8vHbjxo0/FCIZNWpUxfTp009cdtlll4D9RNXIkSMr9+7d22xir/fQQw9l33zzzf0iIiJqhg8fXp6VleUN8Pjjj+fMnj07OjY2dqDFYjEPPvhg9syZM4v+8Y9/ZN5www0X22w2unfvXvv111/vu+222wpXrFjRPS4uLn7IkCHlffr0afT7n2tJwqCgINsVV1xR0bVr17rZs2efc3UyLf3XwowxfHPwBO9sOcYn6ccpqqjFv+GhfWwoPl6aSFuSlv5TZ8rMzPRKTk4ecODAgZ2NXY6lpf/agDqb4ZNduTyfeoDtR4s1kSrVRj377LPdH3vssZ5/+ctfjpzP9a1OJVURWQP8H/ChMcZ2tvXVKTVWG+9sOcYLGw5wML+cmO5+LL5xMNcP7amJVKk2aO7cuSfmzp173jOtOttTfR6YDfxDRN4Alhlj9pxvo51BebWVVd9lsfSLQ+SWVDEwKpBnpw9l4qDINjU7qHKKzWazicViaftjZcrtbDabAE12Lp1KqsaYT4FPRSQIuBlYLyJHgCXAcmNMrSuC7QgKy2t4+ZtMln2dSVFFLT+9KIQnplzKlbGhendT+7UzPz8/PiwsrFgTa+dms9kkPz8/CNjZ1DpOj6mKSHdgBnArsAVYAYwCZgLJFxRpB5BTXMnSLw6x6rssKmrqGHdJBPeM6cdl0U1OBqnaCavVekdubu7S3NzcQehliJ2dDdhptVrvaGoFZ8dU3wLigFeBScaYHMdHr4tIk6flReQ+4E5AgCXGmL+LSAjwOhADZAI3GWMKnYmjLTqQX8aLqQd4e8sxbAauS4hiTnI/+kcEtHZoykWGDRuWB/y8teNQ7YOzPdVnjTGfNfZBU5eiiMgg7Ak1EagBPhKR9x3L/mOMWSwiC4GFwG/POfJWtuNoMc+l7OejXbl08bAwPTGaO6+8SO+9V6qTczapXiIim40xRQAi0g242RjzXHPbAN8aYyoc26QCNwDXcWq44GUghXaUVKutdTz+wR6WfZ1JgI8n9yZfzKyRMYT6N3ozh1Kqk3E2qd5pjPln/RtjTKGI3Ak0l1R3An92jMVWAlcDm4CI+uEDY0yOiIQ3s482JbOgnLmrNrPzWAmzR8awYHx/AnTqEaVUA84mVYuIiHHcfiUiHkCzt6MZY3aLyBPAeqAM2AY0Wj2nMSJyF3AXQHR0tLObuc2727J58K0deFiEJbcNZ3x8RGuHpJRqg5w9k/kxsFpExorIVcAq4KOzbWSM+Zcx5jJjzJXASWAfcFxEIgEcz3lNbPuSMWa4MWZ4WFiYk2G6XmVNHQvXbGfeqi0M6BHAB/ddoQlVKdUkZ3uqvwV+CdyN/Uz+J8DSs20kIuHGmDwRiQZuBC4H+mK/DGux43ntecTdIvYdL+XelZvJOF7GPcn9uH98f7w89IoapVTTnL3434b9rqrnz3H/axxjqrXAvY6x2MXYe723A1nA1HPcp9sZY3gj7SiL1u7E39uTV/47kSv7t15vWSnVfjh7nWos8DgQD/jULzfGNDpLY4PPr2hk2Qlg7LmF2XLKqq08/PYO3tmaTVK/7vz9F0MID/Q5+4ZKKYXzh///Bv4A/A0Yg70OQIe753JXdjFzV27h8IlyFozvz71jLtb79JVS58TZAUJfY8x/sNdfPWyM+SNwlfvCalnGGF79JpMbnvuaihorK+/8KfPGxmpCVUqdM2d7qlUiYgH2ichc4BjQbq4vbU5xZS0L12znw525JA8I46mpCXTXC/mVUufJ2aQ6H/AD5gGPYh8CmOmuoFrKlqxCfrVqC7nFVTx4dRx3jLoIi/ZOlVIX4KxJ1XGh/03GmN9gv4h/ttujagFrtx7j16u3ERHow+o5l2s1KaWUS5w1qRpj6kRkWMM7qtq7qto6Hl2XzuBeQSyblUiQn95qqpRyDWcP/7cAax1V/8vrFxpj3nJLVG72xqYjFJTV8Oz0yzShKqVcytmkGgKc4PQz/gZod0m1ts7GC6kHGdanGyP6hrR2OEqpDsbZO6o6xDgqwLtbszlWVMmj1w/U6U2UUi7n7B1V/8beMz2NMea/XR6RG9lshudTDxDXI4AxAzrEFWFKqTbG2cP/dQ1e+2AvNp3t+nDc65P04+zPK+MfNw/VXqpSyi2cPfxf0/C9iKwCPnVLRG5ijOG5lP3EdPfjmsGRrR2OUqqDOt86drFA61eOPgdf7i9g+9Fifjm6n95+qpRyG2fHVEs5fUw1l3Y0rxTAc58fICLQmxsv69naoSilOjBnD//b9XzLaYcL+ebgCR6+5hK8PT1aO5z2wxioOAElx6D4mP3ZWg1d/MCrwaOLH3j5gldX+3MXx7OnD+jYtepknO2p3gB8ZowpdrwPBpKNMe+4MzhXeT5lP8F+Xtyc2K5GLNzLGKgsPD1hnvm6JBusVRfQiJyedLv4g283xyPY8RzSYJnj4edY5uWnSVm1O86e/f+DMebt+jfGmCIR+QPQ5pPqntwSPt2dx/3j+tPV29mv24FYayB/DxzfCcd32R/FR+wJs7bi9HXFAwKjILAnRA6BuGsgsJd9WVBP+2svH6ithJpy+3Nthf1RU3Hqdf3yM5dVl0JlEZw8ZE/olYVgrWw6do8uDRJvsD3JevqAp3eDZ+8z3vvYt/P0OX2ZTxBEj3Dvz1opzmE21QvYtlU9n3KArl08mJnUp7VDcb+yfDi+A3J32pNo7k4o2As2xyS2nj4QFgcRg6D/hFMJNMiROP0jwOLE8IhPkOtirq08lWArC6HiZIP3DV8XQVWRffjBWmX/Y2GtOvW+rrr5dkL7w9zvXRe3Uk1wNjFuEpGngX9iP2H1KyDNbVG5yOET5by3LZs7r7iIYL9mZ9RuX+pqoWCfI3HuONULLTt+ap2ASEfy/Jn9ucdgCOkHHm3sb6GXr/0RGHVh+zEG6s5ItA0TsEUnbFQtw9nfsF8Bvwded7z/BHjYLRG50IsbDuLpYeH2UX1bO5TzYwwUH4W83ZCXfuqRn3GqZ2bxsvc++13lSJ6DIGIwdO3eurG3NJFTQwFKtSJnz/6XAwvdHItLHS+p4s1NR5k6vFf7mLiv/IQjae6GvF2O591QXXJqnYAoCL8E+o629zwjBtkPaz07UC9cqXbO2bP/64Gpxpgix/tuwGvGmP9yZ3AXYukXB6kzhl9e2a+1QwGbDWrKoKr41OPkwdMTaMNDd59gCI+HS2+yJ9HwePuzrxbSVqqtc/bwP7Q+oQIYYwpFpM1WJCksr2HFxiwmXRpJdHc/1zdQUw4HU+3XcNYnyeqSBkmzwetqx/sf16MBT18Ij4OLxzVInvEQ0EMvJVKqnXI2qdpEJNoYkwUgIjE0miXahpe/yaSipo67ky927Y5ztkHaMtj+BtSUNvhAwDvQflbcJwh8AiG4N3gPPH2ZT5BjvUAI7gPdYpw7266UajecTaoPAV+KSKrj/ZXAXe4J6cKUVVv591eZjI+PYEAPF9wIVl0KO96EzS9D9hb7ZUkDb4Ah06FbX3uC7BKgZ5eVUoDzJ6o+EpHh2BPpVmAt0MxV261n1cYsiitruSf5AsZSjbEn0LRl9oRaWw7hA2HiX+HSqTq2qZRqkrMnqu4A7gN6YU+qPwW+4fTpVVpdtbWOJV8cJKlfd4aez+yoVcWw4w17Ms3dYb+DZ9CNMGw29Bym45xKqbNy9vD/PuAnwLfGmDEiEgf8yX1hnZ81acfIK63mb78Y4vxGxsDRTfZEuust+y2VPQbDNU/B4KmuvXtIKdXhOZtUq4wxVSKCiHgbY/aIyAC3RnaOrHU2Xkg9QELvYJL6OXHhe3UpbF1lT6Z5u+zFPgZPhWGzIGqo9kqVUufF2aR61FGZ6h1gvYgU0samU3l/Rw5ZJyt46JpLzj5VSp0Vlk+BI9/aE+ikZ2DQZPBu1xUOlVJtgLMnqm5wvPyjiHwOBAEfuS2qc2SzGZ77/ACx4f6MvyTi7Bt8+bQ9oV73HAy9xf0BKqU6jXO+DsgYk2qMedcYU3O2dUXkfhHZJSI7RWSViPiIyDIROSQiWx2PcxgAbdxne/LYe7yUe8b0w3K2qVKOboKUxfZDfU2oSikXc1vJIhHpCcwD4o0xlSKyGpjm+Pg3xpg3XdGOMYZ/puynVzdfJl16lkpH1WXw1p32ikhXP+mK5pVS6jTuvmLdE/AVEU/ADzeMw3578CRbsor45eh+eHqc5et8/Dt7geQbXrAXPVZKKRdzW1I1xhwDngSygByg2BjziePjP4vIdhH5m4g0WqtNRO4SkU0isik/P7/Jdp5L2U+ovzdTh/VqPqDd78HmV2DUfIgZdT5fSSmlzsptSdVRyeo6oC8QBXQVkRnA74A47Ne9htDErKzGmJeMMcONMcPDwsIabWP70SK+2FfAHVf0xcermXvoS3Lg3XkQmQDJD17I11JKqWa58/B/HHDIGJNvjKkF3gKSjDE5xq4a+DeQeL4NPPf5AQJ9PLllRDMT+tlssPYe+7QdNy7V2qNKKbdyZ1LNAn4qIn5iv3B0LLBbRCIBHMuuB3aez87zS6v5bG8es5JiCPDxanrF716CA5/Bfz0GYf3PpymllHKa287+G2M2isibwGbACmwBXgI+FJEwQLDXEZhzPvsPC/Am9TfJ+Hk18xWOp8P6RfZJ7obffj7NKKXUOXHrLHDGmD8AfzhjscuKsEQG+Tb9obXafvmUTyD8/Fm97VQp1SLa2NSaLvSfR+yzjE5fDf6Nn+hSSilX65iVlQ+mwDfP2g/5+7fZabSUUh1Qx0uqFSfh7bvts4z+7LHWjkYp1cl0rMN/Y2DdfCjPh5tXQRc3TPqnlFLN6Fg91a0rIX0tXPUQRF1wnRallDpnHSepnjwEH/4P9BkFSfNaOxqlVCfVMZJqnRXeugvEw14sRad9Vkq1ko4xpvrFU3D0O5j8Lwju3drRKKU6sfbfUz3yPaQ+AYNvgsFTWjsapVQn176TasOi09do0WmlVOtr34f/Hy2EwkyY9b5OJa2UahPab0+1JAfS34VR90PMyNaORimlgPbcUw2MhHu+hq7hrR2JUkr9oP0mVYCgs0yhopRSLaz9Hv4rpVQbpElVKaVcSIwxrR3DWYlIPnDYDbsOBQrcsF9tu221e7a2+xhjtOiucol2kVTdRUQ2GWOGa9sdu93Wblt1Lnr4r5RSLqRJVSmlXKizJ9WXtO1O0W5rt606kU49pqrzm7OsAAAGJElEQVSUUq7W2XuqSinlUppUlVLKhTplUhWR3iLyuYjsFpFdInJfC7fvISJbRGRdC7cbLCJvisgex3e/vAXbvt/xs94pIqtExMeNbf2fiOSJyM4Gy0JEZL2I7HM8d3NX+6pz65RJFbACvzbGXAL8FLhXROJbsP37gN0t2F69Z4CPjDFxQEJLxSAiPYF5wHBjzCDAA5jmxiaXARPOWLYQ+I8xJhb4j+O9Ui7XKZOqMSbHGLPZ8boUe3Lp2RJti0gv4BpgaUu016DdQOBK4F8AxpgaY0xRC4bgCfiKiCfgB2S7qyFjzAbg5BmLrwNedrx+GbjeXe2rzq1TJtWGRCQGGApsbKEm/w78D2BrofbqXQTkA/92DD0sFZGuLdGwMeYY8CSQBeQAxcaYT1qi7QYijDE5jnhyAK0ZqdyiUydVEfEH1gDzjTElLdDetUCeMSbN3W01whO4DHjeGDMUKKeFDoEd45fXAX2BKKCriMxoibaVammdNqmKiBf2hLrCGPNWCzU7Evi5iGQCrwFXicjyFmr7KHDUGFPfI38Te5JtCeOAQ8aYfGNMLfAWkNRCbdc7LiKRAI7nvBZuX3USnTKpiohgH1vcbYx5uqXaNcb8zhjTyxgTg/1EzWfGmBbpsRljcoEjIjLAsWgskN4SbWM/7P+piPg5fvZjafkTde8CMx2vZwJrW7h91Um078r/528kcCuwQ0S2OpY9aIz5oBVjagm/AlaISBfgIDC7JRo1xmwUkTeBzdivvNiCG28bFZFVQDIQKiJHgT8Ai4HVInI79iQ/1V3tq85Nb1NVSikX6pSH/0op5S6aVJVSyoU0qSqllAtpUlVKKRfSpKqUUi6kSbUViUiKiLh9MjoRmeeoSrXC3W2d0e4fReSBlmxTqdbWWa9TbfdExNMYY3Vy9XuAicaYQ+6MSSmlPdWzEpEYRy9viaMe6Cci4uv47IeepoiEOm4/RURmicg7IvKeiBwSkbkissBRyORbEQlp0MQMEfnaUWc00bF9V0dN0O8d21zXYL9viMh7wI8Kkjja2Ol4zHcsewF7MZV3ReT+M9b3EJG/OtrZLiK/dCxPFpENIvK2iKSLyAsiYnF8drOI7HC08USDfU0Qkc0isk1E/tOgmXjHz+mgiMxr8P3ed6y7U0R+cSH/Rkq1KcYYfTTzAGKw3wU0xPF+NTDD8ToFe41QgFAg0/F6FrAfCADCgGJgjuOzv2Ev4FK//RLH6yuBnY7Xf2nQRjCQAXR17PcoENJInMOAHY71/IFdwFDHZ5lAaCPb3AU87HjtDWzCXvQkGajCnow9gPXAFOzFULIc38kT+Ax7Cb0w4AjQ17GvEMfzH4GvHfsOBU4AXsDk+u/tWC+otf+d9aEPVz308N85h4wx9bezpmFPtGfzubHXai0VkWLgPcfyHcClDdZbBfYaoCISKCLBwM+wF16pH4/0AaIdr9cbY86sFQowCnjbGFMOICJvAVdgvyW0KT8DLhWRKY73QUAsUAN8Z4w56NjXKsf+a4EUY0y+Y/kK7H8M6oANxjG8cEZ87xtjqoFqEckDIhw/gycdPd11xpgvmolRqXZFk6pzqhu8rgN8Ha+tnBpCOXN6kIbb2Bq8t3H6z/3M+4QNIMBkY8zehh+IyAjsJfsaI00F3wwBfmWM+fiMdpKbiaup/TR1v/OZPztPY0yGiAwDrgYeF5FPjDGPnGvwSrVFOqZ6YTKxH3aD/fD4fPwCQERGYS/eXAx8DPzKUdEJERnqxH42ANc7KkF1BW4AztYD/Bi421EGERHp36BwdaKI9HWMpf4C+BJ7Ie/RjvFjD+BmIBX4xrG8r2M/IWc21JCIRAEVxpjl2ItXt1QJQqXcTnuqF+ZJ7JWPbsU+vng+CkXkayAQ+G/HskexzxCw3ZFYM4Frm9uJMWaziCwDvnMsWmqMae7QH+xTusQAmx3t5HNqmpFvsFd2Gow9Yb9tjLGJyO+Az7H3Tj8wxqwFEJG7gLccSTgPGN9Mu4OBv4qIDfuQwt1niVOpdkOrVKkfcRz+P2CMaTaRK6V+TA//lVLKhbSnqpRSLqQ9VaWUciFNqkop5UKaVJVSyoU0qSqllAtpUlVKKRf6f7pW30x+XFkdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#optimizer= torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "val_acc_list=[]\n",
    "train_acc_list=[]\n",
    "for epoch in range(num_epochs):\n",
    "    #linear annealing of learning rate at every 4th epoch\n",
    "    if epoch%3==2:\n",
    "       optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate*0.5)\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            train_acc= test_model(train_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "    val_acc_list.append(val_acc)\n",
    "    train_acc_list.append(train_acc)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "epochs=[i for i in range(1,num_epochs+1)]\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot( epochs,train_acc_list, label=\"training accuracy\")\n",
    "plt.plot( epochs, val_acc_list, label=\"validation accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"number of epochs\")\n",
    "# Place a legend to the right of this smaller subplot.\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Train Acc 94.32\n",
      "Val Acc 87.1\n",
      "Test Acc 84.868\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print(\"Train Acc {}\".format(test_model(train_loader, model)))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to print 3 correctly and incorrectly classified examples\n",
    "\n",
    "import random\n",
    "\n",
    "def print3(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    #variable to count correctly classified and printed\n",
    "    c=0\n",
    "    \n",
    "    #variable to count incorrectly classified and printed\n",
    "    ic=0\n",
    "    model.eval()\n",
    "    loop_count=0\n",
    "    for data, lengths, labels in loader:\n",
    "        #print(labels.shape)\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        #variable to decide whether to consider this batch or not\n",
    "        this=random.randint(0,1)\n",
    "        \n",
    "        if this==1:\n",
    "            if c<3:\n",
    "                for i in range(len(predicted)):\n",
    "                    if predicted[i]==label_batch[i] and c<3:\n",
    "                        print(\"Correctly Classified no.\"+str(c+1))\n",
    "                        review=\"\"\n",
    "                        for id in data_batch[i]:\n",
    "                            review+=id2token[id.item()]+\" \"\n",
    "                            #print(id)\n",
    "                        print(\"Review:\",review)\n",
    "                        if labels[i].item()==1:\n",
    "                            ans=\"pos\"\n",
    "                        else:\n",
    "                            ans=\"neg\"\n",
    "                        print(\"Actual label:\",ans)\n",
    "                        if predicted[i].item()==1:\n",
    "                            ans=\"pos\"\n",
    "                        else:\n",
    "                            ans=\"neg\"\n",
    "                        print(\"Predicted label:\",ans,\"\\n\\n\")\n",
    "                        c+=1\n",
    "            if ic<3:\n",
    "                for i in range(len(predicted)):\n",
    "                    if predicted[i]!=label_batch[i] and ic<3:\n",
    "                        print(\"Incorrectly classified no.\"+str(ic+1))\n",
    "                        review=\"\"\n",
    "                        for id in data_batch[i]:\n",
    "                            review+=id2token[id.item()]+\" \"\n",
    "                            #print(id)\n",
    "                        print(\"Review:\",review)\n",
    "                        if label_batch[i].item()==1:\n",
    "                            ans=\"pos\"\n",
    "                        else:\n",
    "                            ans=\"neg\"\n",
    "                        print(\"Actual label:\",ans)\n",
    "                        if predicted[i].item()==1:\n",
    "                            ans=\"pos\"\n",
    "                        else:\n",
    "                            ans=\"neg\"\n",
    "                        print(\"Predicted label:\",ans,\"\\n\\n\")\n",
    "                        ic+=1\n",
    "        if ic==3 and c==3:\n",
    "            break\n",
    "        loop_count+=1\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly Classified no.1\n",
      "Review: 1st watch <unk> 4 out of <unk> jim <unk> brian smith drab and un spectacular suppos sequel to the origin classic anim 101 dalmatian ye the movi continu where it end in the first one but the problem is that it play out much like the origin one of the great thing about the origin wa the pace of the stori which thi one doe n't have the anim is also veri un spectacular for disney and all we get is the same charact go thru the same kind of stori all over again when is disney go to stop bore us with sequel and re do 's etc .. etc probabl when we stop rent or buy thi mediocr fare that they have put out <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "Actual label: neg\n",
      "Predicted label: neg \n",
      "\n",
      "\n",
      "Correctly Classified no.2\n",
      "Review: after read the comment to thi movi and see the mix review i decid that i would add my ten cent worth to say i thought the film wa excel not onli in the visual beauti the write music score act and direct but in put across the stori of joseph smith and the road he travel through life of hardship and persecut for believ in god the way he felt and knew to be hi path i am veri pleas inde to have had a small part in tell the stori of thi remark man i recommend everyon to see thi when the opportun present itself no matter what religi path he or she may be walk thi onli <unk> one with more determin to live the life that we should with true valu of love and forgiv as the savior taught us to do <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "Actual label: pos\n",
      "Predicted label: pos \n",
      "\n",
      "\n",
      "Correctly Classified no.3\n",
      "Review: i have to say that the event of 9/11 did n't hit me until i saw thi documentari it took me a year to come to grip with the devast i wa the one who wa chang the station on the radio and channel on tv if there wa ani talk about the tower i wa sick of hear about it when thi wa air on tv a year and a day later i wa <unk> my eye out it wa the first time i had cri sinc the attack i highli recommend thi documentari i am watch it now on tv 5 year later and i am still cri over the tragedi the fact that thi contain one of the onli video shot of the first plane hit the tower is amaz it wa an accid and look where it got them these two brother make me want to have been there to help <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "Actual label: pos\n",
      "Predicted label: pos \n",
      "\n",
      "\n",
      "Incorrectly classified no.1\n",
      "Review: i thought the origin of thi film wa quaint and charm as well as have me sit on the edg of my seat tri to figur it out.<br />sinc i had alreadi seen the origin when i saw thi on sci fi <unk> i do n't know if thi remak wa deliber made for sci fi i knew what it wa within the first few minut sinc i like richard <unk> as a charact actor i want to see how he would pull it off.<br />the writer produc etc modern the film a bit by tri to explain the plight of the alien they could no longer reproduc their own kind and need help use the same pseudo scienc that ha been cram in our ear in the 90 's mayb it ad a bit of polish to the film or not.<br />thi film film thi product take on a more sinist edg than the origin <unk> the origin end with a confront between the young woman and the alien and an understand of sort took place although no resolut of the alien 's <unk> />i sort of rememb that in thi remak the woman becam rather hostil toward the \n",
      "Actual label: pos\n",
      "Predicted label: neg \n",
      "\n",
      "\n",
      "Incorrectly classified no.2\n",
      "Review: it 's terrif when a funni movi doe n't make smile you what a piti thi film is veri bore and so long it 's simpli <unk> the stori is stagger without goal and no fun.<br />you feel better when it 's finish <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "Actual label: neg\n",
      "Predicted label: pos \n",
      "\n",
      "\n",
      "Incorrectly classified no.3\n",
      "Review: everi onc in a while someon out of the blue look at me a littl sideway and ask what 's with <unk> i know immedi they have a case of bare hidden amus horror you see i wa the cinematograph on the film.<br />let me clarifi some point regard thi interest life <unk> <unk> <unk> wa call one hard hit i met jame cahil in juli of 1999 a day after i wrap triangl squar a great littl 35 mm featur that like so mani indi featur of the era never got distribut despit festiv accolad ... it fell etern victim to the fine print of <unk> 's notori experiment featur contract but i digress ... />i though i wa on a roll and when jame ask me to shoot hi littl gangster flick in 16 mm with a shoot budget of about <unk> not want to break pace i took it after all clerk el <unk> ... i too believ the myth back <unk> />let 's just chalk it up as film school for mani involv myself includ <unk> wa shot over two week in august 1999 in <unk> <unk> and santa <unk> ca cahil taught drama at a \n",
      "Actual label: neg\n",
      "Predicted label: pos \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print3(val_loader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
